title	type	url_slug	venue	date	location	talk_url	description
Neural Network Design: Learning from Neural Architecture Search	Talk	talk-neural-network-design	SSCI 2020	2020-12-02	Canberra, Australia	https://youtu.be/DnP05lbE7dM	Neural Architecture Search (NAS) aims to optimize deep neural networks’ architecture for better accuracy or smaller computational cost and has recently gained more research interests. Despite various successful approaches proposed to solve the NAS task, the landscape of it, along with its properties, are rarely investigated. In this paper, we argue for the necessity of studying the landscape property thereof and propose to use the so-called Exploratory Landscape Analysis (ELA) techniques for this goal. Taking a broad set of designs of the deep convolutional network, we conduct extensive experimentation to obtain their performance. Based on our analysis of the experimental results, we observed high similarities between well-performing architecture designs, which is then used to significantly narrow the search space to improve the efficiency of any NAS algorithm. Moreover, we extract the ELA features over the NAS landscapes on three common image classification data sets, MNIST, Fashion, and CIFAR-10, which shows that the NAS landscape can be distinguished for those three data sets. Also, when comparing to the ELA features of the well-known Black-Box Optimization Benchmarking (BBOB) problem set, we found out that the NAS landscapes surprisingly form a new problem class on its own, which can be separated from all 24 BBOB problems. Given this interesting observation, we, therefore, state the importance of further investigation on selecting an efficient optimizer for the NAS landscape as well as the necessity of augmenting the current benchmark problem set.   *Keywords*: Neural Architecture Search, AutoML, Deep Learning, Exploratory Landscape Analysis
An Incremental Algorithm for Repairing Training Sets with Missing Values	Talk	talk-iari-ipmu	IPMU 2016	2016-11-02	Eindhoven, The Netherlands	https://link.springer.com/chapter/10.1007/978-3-319-40581-0_15	Real-life datasets that occur in domains such as industrial process control, medical diagnosis, marketing, risk management, often contain missing values. This poses a challenge for many classification and regression algorithms which require complete training sets. In this paper we present a new approach for “repairing” such incomplete datasets by constructing a sequence of regression models that iteratively replace all missing values. Additionally, our approach uses the target attribute to estimate the values of missing data. The accuracy of our method, Incremental Attribute Regression Imputation, IARI, is compared with the accuracy of several popular and state of the art imputation methods, by applying them to five publicly available benchmark datasets. The results demonstrate the superiority of our approach.
Analysis and Visualization of Missing Value Patterns	Talk	talk-misvis-ipmu	IPMU 2016	2016-11-02	Eindhoven, The Netherlands	https://link.springer.com/chapter/10.1007/978-3-319-40581-0_16	Missing values in datasets form a very relevant and often overlooked problem in many fields. Most algorithms are not able to handle missing values for training a predictive model or analyzing a dataset. For this reason, records with missing values are either rejected or repaired. However, both repairing and rejecting affects the dataset and the final results, creating bias and uncertainty. Therefore, knowledge about the nature of missing values and the underlying mechanisms behind them are of vital importance. To gain more in-depth insight into the underlying structures and patterns of missing values, the concept of Monotone Mixture Patterns is introduced and used to analyze the patterns of missing values in datasets. Several visualization methods are proposed to present the “patterns of missingness” in an informative way. Finally, an algorithm to generate missing values in datasets is provided to form the basis of a benchmarking tool. This algorithm can generate a large variety of missing value patterns for testing and comparing different algorithms that handle missing values.
Local Subspace-Based Outlier Detection using Global Neighbourhoods	Talk	talk-gloss-bigdata	IEEE Big Data	2016-12-07	Washington DC, USA	https://ieeexplore.ieee.org/document/7840717	Outlier detection in high-dimensional data is a challenging yet important task, as it has applications in, e.g., fraud detection and quality control. State-of-the-art density-based algorithms perform well because they 1) take the local neighbourhoods of data points into account and 2) consider feature subspaces. In highly complex and high-dimensional data, however, existing methods are likely to overlook important outliers because they do not explicitly take into account that the data is often a mixture distribution of multiple components. We therefore introduce GLOSS, an algorithm that performs local subspace outlier detection using global neighbourhoods. Experiments on synthetic data demonstrate that GLOSS more accurately detects local outliers in mixed data than its competitors. Moreover, experiments on real-world data show that our approach identifies relevant outliers overlooked by existing methods, confirming that one should keep an eye on the global perspective even when doing local outlier detection.
Fuzzy Clustering for Optimally Weighted Cluster Kriging	Talk	talk-owck-wcci	WCCI 2016	2016-07-25	Vancouver, Canada	https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7737789&casa_token=5Wt3ADr5Y48AAAAA:dLoSWvxKqsPJqcyWtrq-cRt4ng1agjRsO2MT8BUy92ORPwJgPAilqslHkXBBKv08-uDB1kgTsdmn	Kriging or Gaussian Process Regression has been successfully applied in many fields. One of the major bottlenecks of Kriging is the complexity in both processing time (cubic) and memory (quadratic) in the number of data points. To overcome these limitations, a variety of approximation algorithms have been proposed. One of these approximation algorithms is Optimally Weighted Cluster Kriging (OWCK). In this paper, OWCK is extended and enhanced by the use of fuzzy clustering methods in order to increase the accuracy. Several options are proposed and evaluated against both the original OWCK and a variety of other Kriging approximation algorithms.
From Data to Knowledge to Optimization	Invited talk	invited-phd	PhD Colloquium	2017-02-02	Leiden, The Nethehrlands		Invited talk for thhe PhH Colloquium with Math and Informatics
A Novel Uncertainty Quantification Method for Efficient Global Optimization	Talk	talk-uq-ipmu	IPMU 2018	2018-06-06	Cádiz, Spain	https://link.springer.com/chapter/10.1007/978-3-319-91479-4_40	For most regression models, their overall accuracy can be estimated with help of various error measures. However, in some applications it is important to provide not only point predictions, but also to estimate the “uncertainty” of the prediction, e.g., in terms of confidence intervals, variances, or interquartile ranges. There are very few statistical modeling techniques able to achieve this. For instance, the Kriging/Gaussian Process method is equipped with a theoretical mean squared error. In this paper we address this problem by introducing a heuristic method to estimate the uncertainty of the prediction, based on the error information from the k-nearest neighbours. This heuristic, called the k-NN uncertainty measure, is computationally much cheaper than other approaches (e.g., bootstrapping) and can be applied regardless of the underlying regression model. To validate and demonstrate the usefulness of the proposed heuristic, it is combined with various models and plugged into the well-known Efficient Global Optimization algorithm (EGO). Results demonstrate that using different models with the proposed heuristic can improve the convergence of EGO significantly.
Deep Steel: Optimizing networks to discover defects on steel surface	Invited talk	invited-ecole	ECOLE workshop	2018-11-29	Leiden, The Netherlands		Invited talk for the ECOLE program, learning to optimize.
Deep Steel: Optimizing networks to discover defects on steel surface	Invited talk	invited-tata	Tata Steel	2019-01-30	IJmuiden, The Netherlands		Invited talk at Tata Steel, about using deep learning to discover steel surface defects.
Automatic Configuration of Deep Neural Networks with Parallel Efficient Global Optimization	Talk	talk-cnn-ijcnn	IJCNN 2019	2019-07-02	Budapest, Hungary	https://ieeexplore.ieee.org/abstract/document/8851720?casa_token=W7U_7pA_Ad8AAAAA:7rq1Z4nAqShtzp_-5AffRG-h0e12dq4qClDmI6i05_4V7GXv_m23EAVvqhapyh25tio9XMIoBRA0	Designing the architecture for an artificial neural network is a cumbersome task because of the numerous parameters to configure, including activation functions, layer types, and hyper-parameters. With the large number of parameters for most networks nowadays, it is intractable to find a good configuration for a given task by hand. In this paper the Mixed Integer Parallel Efficient Global Optimization (MIP-EGO) algorithm is proposed to automatically configure convolutional neural network architectures. It is shown that on several image classification tasks this approach is able to find competitive network architectures in terms of prediction accuracy, compared to the best hand-crafted ones in literature, when using only a fraction of the number of training epochs. Moreover, instead of the standard sequential evaluation in EGO, several candidate architectures are proposed and evaluated in parallel, which reduces the execution overhead significantly and leads to an efficient automation for deep neural network design.
Deep Learning, A broad introduction	Invited talk	invited-ecole-2	Ecole Summer school	2019-07-30	Leiden, The Netherlands		Invited talk for the Summer school for Early Stage Researchers. This summer school takes place within the EU project Experience-based Computation: Learning to Optimise (ECOLE), which investigates novel synergies between nature-inspired optimisation and machine learning to address key challenges faced by the European industry.
A New Approach Towards the Combined Algorithm Selection and Hyper-parameter Optimization Problem 	Talk	talk-cash-ssci	SSCI 2019	2019-12-08	Xiamen, China	https://ieeexplore.ieee.org/abstract/document/9003174	Machine learning algorithms often have many hyper-parameters that can be tuned to improve empirical performance. However, manually exploring the complex search spaces is tedious and cannot guarantee to find satisfactory outcomes. Recently, the Efficient Global Optimization (EGO) algorithm for solving the hyper-parameter optimization problem have shown substantial improvements. However, these algorithms cannot easily be expanded to include the selection of an algorithm. Automatically determining and optimizing an algorithm is known as the combined algorithm selection and hyper-parameter optimization (CASH) problem. In this paper, a novel mixed integer efficient global optimization algorithm and its variants are proposed to solve the CASH problem efficiently. The proposed algorithm is compared with a wide set of state-of-the-art algorithms in both a Black Box Optimization and a CASH problem setting. For the CASH problem setup, seven machine learning algorithms are optimized for a large set of classification t asks. Results show that the proposed algorithms outperform on the black box optimization task and can also outperform alternative state-of-the-art approaches on the CASH problem for specific instances.
Automated Deep Learning 	Invited talk	invited-sails-1	SAILS 2020	2020-03-11			Invited presentation at SAILS (online)
Neural Architecture Search VS Black Box Optimization	Invited talk	invited-bmw	BMW invited presentation	2021-11-22	Munich, Germany		Invited presentation at BMW headquarters
Datamining & Apps 	Invited guest lecture	lecture-datascience	Data Science course, Liacs	2016-09-23	Leiden, The Netherlands		Invited lecture at the Data Science course, LIACS, Leiden University
Fuzzy Clustering for Optimally Weighted Cluster Kriging	Invited talk	invited-fuzzy	PhD Seminar, Liacs	2016-07-25	Leiden, The Netherlands		Invited talk for the PhD Seminar at LIACS, Leiden University
Datamining & Apps 	Invited talk	invited-cwi	CWI	2015-03-10	Amsterdam, The Netherlands		Invited talk at the CWI, Amsterdam
Optimally Weighted Cluster Kriging	talk	talk-owck-ida	IDA 2015	2015-10-22	Saint-Etienne, France		In business and academia we are continuously trying to model and analyze complex processes in order to gain insight and optimize. One of the most popular modeling algorithms is Kriging, or Gaussian Processes. A major bottleneck with Kriging is the amount of processing time of at least O(n3)  and memory required O(n2)  when applying this algorithm on medium to big data sets. With big data sets, that are more and more available these days, Kriging is not computationally feasible. As a solution to this problem we introduce a hybrid approach in which a number of Kriging models built on disjoint subsets of the data are properly weighted for the predictions. The proposed model is both in processing time and memory much more efficient than standard Global Kriging and performs equally well in terms of accuracy. The proposed algorithm is better scalable, and well suited for parallelization.
BIAS: A Toolbox for Benchmarking Structural Bias in the Continuous Domain	talk	talk-gecco-bias	GECCO 2023	2023-07-15	Lisbon, Portugal	https://youtu.be/tVBM56y-lU0	Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. Most benchmarks are performance based, to test algorithm performance under a wide set of conditions. There is also resource- and behavior-based benchmarks to test the resource consumption and the behavior of algorithms. In this article, we propose a novel behavior-based benchmark toolbox: BIAS (Bias in algorithms, structural). This toolbox can detect structural bias (SB) per dimension and across dimension-based on 39 statistical tests. Moreover, it predicts the type of SB using a random forest model. BIAS can be used to better understand and improve existing algorithms (removing bias) as well as to test novel algorithms for SB in an early phase of development. Experiments with a large set of generated SB scenarios show that BIAS was successful in identifying bias. In addition, we also provide the results of BIAS on 432 existing state-of-the-art optimization algorithms showing that different kinds of SB are present in these algorithms, mostly toward the center of the objective space or showing discretization behavior. The proposed toolbox is made available open-source and recommendations are provided for the sample size and hyper-parameters to be used when applying the toolbox on other algorithms.